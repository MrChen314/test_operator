要求1：
1.生成.cuh\.cu文件(cu文件命名mla_bwd.cu)
    1.1其中/Users/chenql/Desktop/workspace/operator/test_operator/mla_bwd/mla_bwd.cuh，cuh文件大部分已经写好，还需要更改输入输出配置
        输入：q、kv、dO
        输出：q_out、kv_out、dO_out、P、dP
    1.2输入从test_mla_bwd.py生成
        每个cta从全局内存读取输入的规则
        Q：cta0读取前[B_H/2, D_Q]，cta1读取后[B_H/2, D_Q]
        KV：cta0读取前[B_TOPK/2, D_K]，cta1读取后[B_TOPK/2, D_K]
        dO：cta0读取前[B_H/2, D_V]，cta1读取后[B_H/2, D_V]
        从全局内存读取后，直接用寄存器写入到SMEM
        然后cta0和cta1将自己的SMEM中的Q、KV、dO读取并写出到全局内存
    1.3使用TiledMMA_P计算P
        矩阵A：plan.u.q_kv.q
        矩阵B：plan.u.q_kv.kv
        矩阵C：tmem_cols.P
        注意：对tmem_cols.P执行clear_accum
    1.4使用TiledMMA_dP计算dP
        矩阵A：plan.dO
        矩阵B：plan.u.q_kv.kv中的v部分
        矩阵C：tmem_cols.dP
        注意：对tmem_cols.dP执行clear_accum
    1.5mma计算完成后，读取tmem结果，将P和dP存到全局内存
    1.6对.cu文件分块添加finish打印，打印时添加if (cta_idx==0 and blockidx.x==0 and tid==0)的条件，从finish 1 到finish n，方便debug代码卡在哪里
2.生成test_mla_bwd.py精度对比脚本
    2.1生成输入数据
        q_shape:[128, 576]
        kv_shape:[64, 576]
        do_shape:[128, 512]
    2.2对比Q\K\dO精度误差
    2.3对比cuda的结果和torch的matmul计算的P和dP精度误差
3.生成setup安装脚本
4.编译生成.o文件，编译时max_jobs=192
5.执行test_mla_bwd.py进行精度测试


要求2：
以下代码先不考虑mask
1.kernel新增输入、输出、barrier
    输入：lse、O
    输出：s、ds
    barrier
        WG3计算好P后，通知WG0
        WG3计算好dP后，通知WG0
        WG0存好s后，告知WG3
        WG0存好ds后，告知WG3
2.预计算delta：使用/Users/chenql/Desktop/workspace/operator/FlashMLA/csrc/sm100/prefill/sparse/bwd/head128/preprocess_delta.cuh
    可参考/Users/chenql/Desktop/workspace/operator/FlashMLA/csrc/sm100/prefill/sparse/bwd/head128/phase1.cuh 中的run_bwd_preprocess_delta_kernel<D_QK>(params);
3.将mla_bwd.cuh中现有的mma操作的代码包在WG3(warpgroup3)中
4.tilelang反向流程：/Users/chenql/Desktop/workspace/operator/tilelang/examples/deepseek_v32/sparse_mla_bwd.py
    理解tilelang反向中acc_p和acc_dp的计算公式
5.新增WG0计算acc_p和acc_dp
    2.1WG3计算好P后，通知WG0；WG3计算好dP后，通知WG0
    2.2等待WG3计算好p
    2.3使用ku::tmem_ld_32dp32bNx指令将tmem_cols.p读取到寄存器，然后计算softmax(p) -> 通过exp2(P*scale - LSE)获得softmax值s(fp32)
    2.4将s存到plan.s_ds.s，存入的是bf16格式。存好后，告知WG3信息：s已经准备好
    2.5cta0和cta1将自己的SMEM中的s写出到全局内存
    2.6从全局内存读取需要的delta
    2.7等待WG3计算好dp
    2.8计算ds，ds = s * (dp - delta) * scale；注意公式里的s用2.3中算好的fp32格式的值，而不是存到SMEM里面的bf16格式
    2.9.将ds从fp32转换到bf16，存储ds到plan.s_ds.ds，存好后，告知WG3信息：ds已经准备好
    2.10cta0和cta1将自己的SMEM中的ds写出到全局内存
6.test_mla_bwd.py添加精度测试
    6.1生成lse和O的随机输入
    6.2torch先计算delta，然后生成torch版本的计算s和ds的代码
    6.3添加torch版s和ds与cuda输出的s和ds的精度对比


要求3：
1.kernel新增输出、barrier
    输出：dKV
    barrier：
        WG3通知WG2：dKV_part0计算完成
        WG3通知WG2：dKV_part1计算完成
        WG3通知WG2：dKV_part2计算完成
        WG2通知WG3：dKV_part0传输完成
        WG2通知WG3：dKV_part1传输完成
        WG2通知WG3：dKV_part2传输完成
2.WG3新增流程
    2.1等待WG0计算好ds
    2.2WG3使用TiledMMA_dKV计算dV前256维
        矩阵A：plan.s_ds.s，使用SmemLayoutS
        矩阵B：plan.dO，使用SmemLayoutdOTransposed，使用flat_divide切tile[64, 256]，使用第0个分片
        矩阵C：tmem_cols.dKV
        注意：对tmem_cols.dKV执行clear_accum
    2.3WG3使用TiledMMA_dKV计算dK前256维
        矩阵A：plan.s_ds.ds，使用SmemLayoutdS
        矩阵B：plan.u.q_kv.q，使用SmemLayoutQNoPETransposed，使用flat_divide切tile[64, 256]，使用第0个分片
        矩阵C：tmem_cols.dKV
        注意：对tmem_cols.dKV执行累加
    2.4WG3通知WG2：dKV_part0计算完成
    2.5等待WG2传输完dKV_part0
    2.6使用TiledMMA_dKV计算dV后256维
        矩阵A：plan.s_ds.s，使用SmemLayoutS
        矩阵B：plan.dO，使用SmemLayoutdOTransposed，使用flat_divide切tile[64, 256]，使用第1个分片
        矩阵C：tmem_cols.dKV
        注意：对tmem_cols.dKV执行clear_accum
    2.7使用TiledMMA_dKV计算dK后256维
        矩阵A：plan.s_ds.ds，使用SmemLayoutdS
        矩阵B：plan.u.q_kv.q，使用SmemLayoutQNoPETransposed，使用flat_divide切tile[64, 256]，使用第1个分片
        矩阵C：tmem_cols.dKV
        注意：对tmem_cols.dKV执行累加
    2.8WG3通知WG2：dKV_part1计算完成
    2.9等待WG2传输完dKV_part1
    2.10使用TiledMMA_dKV_RoPE计算dK_rope
        矩阵A：plan.s_ds.ds，使用SmemLayoutdS
        矩阵B：plan.u.q_kv.q_rope，使用SmemLayoutQRoPETransposed
        矩阵C：tmem_cols.dKV_RoPE
        注意：对tmem_cols.dKV_RoPE执行clear_accum
    2.11WG3通知WG2：dKV_part2计算完成
    2.12等待WG2传输完dKV_part2
3.新增WG2
    3.1等待WG3计算好dKV_part0
    3.2进行dKV_part0到全局内存的传输(0-255维)
        指令：atom.add.v4
        数据：读取tmem_cols.dKV
    3.3告知WG3信息：dKV_part0已经传输完成
    3.4等待WG3计算好dKV_part1
    3.5进行dKV_part1到全局内存的传输(256-511维)
        指令：atom.add.v4
        数据：读取tmem_cols.dKV
    3.6告知WG3信息：dKV_part1已经传输完成
    3.7等待WG3计算好dKV_part2
    3.8进行dKV_part2到全局内存的传输(512-575维)
        指令：atom.add.v4
        数据：读取tmem_cols.dKV_RoPE
    3.9告知WG3信息：dKV_part2已经传输完成
4.test_mla_bwd.py添加精度测试
    4.1torch_ref计算dKV_ref，计算流程可参考/Users/chenql/Desktop/workspace/operator/tilelang/examples/deepseek_v32/sparse_mla_bwd.py
    4.2对比dKV_cuda和dKV_ref

要求4：
1.kernel新增输出、barrier
    输出：dQ
    barrier：
        cp_async传输kv_peer
        WG1通知WG3：kv_peer加载完成
        WG3通知WG0：dQ计算完成
2.WG0新增流程
    2.1等待WG3计算好dQ
    2.2将dQ传输到全局内存
3.WG1新增流程
    2.1加载对端的kv_peer，参考/Users/chenql/Desktop/workspace/operator/test_operator/smem_cp_async/test_smem_cp_async.cu
    2.2通知WG3：kv_peer加载完成
4.WG3新增流程
    以下新增操作插入到WG3通知WG2：dKV_part0计算完成之后，等待WG2传输完dKV_part0之前
    3.1使用TiledMMA_dQ和TiledMMA_dQ_RoPE计算dQ+=ds@k_local，累加
    3.2等待WG1传输完kv_peer
    3.3使用TiledMMA_dQ和TiledMMA_dQ_RoPE计算dQ+=ds@k_peer，累加
    3.4通知WG0：dQ计算完成
5.test_mla_bwd.py增加对dQ的精度对比，torch_ref = ds@K

注：所有生成的文件都在/test_operator/mla_bwd 目录下